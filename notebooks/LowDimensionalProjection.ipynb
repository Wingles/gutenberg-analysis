{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import umap\n",
    "import numba\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../../gutenberg/src\")\n",
    "from metaquery import meta_query\n",
    "from data_io import get_dict_words_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 16\n",
    "mpl.rcParams['axes.labelsize'] = 14\n",
    "mpl.rcParams['legend.fontsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 12\n",
    "mpl.rcParams['ytick.labelsize'] = 12\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UMAP with a precomputed distance matrix\n",
    "This is the simple approach but computing and storing a $50k \\times 50k$ matrix is no joke.\n",
    "\n",
    "### Using UMAP with a custom distance function\n",
    "Here we let UMAP handle the distances efficiently. Instead we need to pass:\n",
    "+ a **sparse** samples-features matrix\n",
    "+ a numba-jitted custom distance function\n",
    "\n",
    "Our features are word types, so the only way to hold the samples-features matrix is with sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2doc(paths):\n",
    "    \"\"\"\n",
    "    Create a words-to-documents sparse matrix.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    paths : list[str]\n",
    "        List of paths to count files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    MM : scipy.sparse.csr_matrix\n",
    "        A csr sparse matrix with documents in rows and words in columns. \n",
    "        Entries are (relative) frequencies\n",
    "        \n",
    "    \"\"\"\n",
    "    !rm /tmp/PGtmp.txt\n",
    "    for path in paths:\n",
    "        ! cat {path} >> /tmp/PGtmp.txt\n",
    "\n",
    "    with open(\"/tmp/PGtmp.txt\",'r') as f:\n",
    "        x=f.readlines()\n",
    "    W = list(set([xx.split(\"\\t\")[0] for xx in x]))\n",
    "\n",
    "    M = dok_matrix((len(paths),len(W)))\n",
    "    # initialize map from words to integers\n",
    "    w_idx = {v:k for k,v in dict(enumerate(W)).items()}\n",
    "\n",
    "    for d, path in enumerate(paths):\n",
    "        w, f = get_w_f(path)\n",
    "\n",
    "        # add words to sparse matrix\n",
    "        for word,freq in zip(w, f):\n",
    "            M[d,w_idx[word]] = freq\n",
    "\n",
    "    return M.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def D_alpha_max(H1,H2,pi1,pi2,alpha=1.0):\n",
    "    '''The maximum Jensen-Shanon-divergence of two probability distributions p1 and p2\n",
    "    with entropies H1 and H2 and weights pi1 and pi2.\n",
    "    The maximum jsd is obtained by assuming the support of both distributions is disjunct.\n",
    "    For more information see:\n",
    "    Gerlach, Font-Clos, Altmann, Phys. Rev. X 6 (2016) 021009\n",
    "    https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.021009\n",
    "\n",
    "    INPUT:\n",
    "    - H1, float, alpha-entropy of p1\n",
    "    - H2, float, alpha-entropy of p2\n",
    "    - pi1, float, weight given to p1\n",
    "    - pi2, float, weight given to p2\n",
    "    optional\n",
    "    - alpha, float (default:1.0)\n",
    "\n",
    "    OUTPUT:\n",
    "    - D_max, float - maximum JSD\n",
    "    '''\n",
    "    D_max = 0.0\n",
    "    if alpha == 1.0:\n",
    "        D_max = -pi1*np.log(pi1)-pi2*np.log(pi2)\n",
    "    else:\n",
    "        D_max = (pi1**alpha-pi1)*H1+\\\n",
    "                (pi2**alpha-pi2)*H2+\\\n",
    "                1.0/(1.0-alpha)*(pi1**alpha+pi2**alpha-1)\n",
    "    return D_max\n",
    "\n",
    "@numba.njit()\n",
    "def H_alpha_sparse_csr(arr_p,alpha=1.0):\n",
    "    '''Calculate generalized entropy of order-alpha\n",
    "        H_{\\alpha}(\\vec{p}) = \\frac{1}{1-\\alpha}( \\sum_i p_i^{\\alpha} - 1  )\n",
    "        https://en.wikipedia.org/wiki/Tsallis_entropy\n",
    "\n",
    "        INPUT:\n",
    "        - arr_p, array (normalized probability distribution)\n",
    "        optional\n",
    "        - alpha, float (default:1.0 == Boltzmann-Shannon-Gibbs entropy)\n",
    "        OUT:\n",
    "        - H_alpha, float\n",
    "\n",
    "        Note that we have to consider special cases \n",
    "        - alpha=0; if p_i=0: p_i^0=0 and p_i>0: p_i^0=1  \n",
    "        - alpha=1; H --> -\\sum_i p_i log(p_i) and 0*log(0) = 0\n",
    "    '''\n",
    "    ## consider only entries with p>0\n",
    "    arr_p_pos = arr_p\n",
    "    H = 0.0\n",
    "    if alpha == 0.0:\n",
    "        H = len(arr_p_pos) - 1\n",
    "    elif alpha == 1.0:\n",
    "        H = -np.sum(arr_p_pos*np.log(arr_p_pos))\n",
    "    else:\n",
    "        H = 1.0/(1.0-alpha)*(np.sum(arr_p_pos**alpha) - 1.0)\n",
    "    return H\n",
    "\n",
    "def D_alpha_sparse_csr(arr_p1,arr_p2,alpha=2.0,pi1=0.5,normalized=True):\n",
    "    '''\n",
    "    Generalized Jensen-Shannon divergence proposed in:\n",
    "        Gerlach, Font-Clos, Altmann, Phys. Rev. X 6 (2016) 021009\n",
    "        https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.021009\n",
    "\n",
    "    quantifying the divergence between probability distributions p1 and p2\n",
    "\n",
    "    INPUT:\n",
    "    - p1, array  \n",
    "    - p2, array\n",
    "    optional\n",
    "    - alpha, float (default:1)\n",
    "    - pi1, float (default:0.5); weight given to p1 (pi2 = 1-pi1)\n",
    "    - normalized (default:False); if True return normalized version of alpha-JSD\n",
    "\n",
    "    OUTPUT:\n",
    "    - JSD, float\n",
    "\n",
    "    Note that p1 and p2 have to be defined over the SAME support, i.e.\n",
    "    for any index i the probabilities p1[i] and p2[i] refer to the same symbol.\n",
    "    If, for example a symbol j only appears in p1, then p1[j]>0 and p2[j]=0 (and vice versa)\n",
    "    '''\n",
    "\n",
    "    pi2 = 1.0-pi1\n",
    "\n",
    "    H_1 = H_alpha_sparse_csr(arr_p1.data,alpha=alpha)\n",
    "    H_2 = H_alpha_sparse_csr(arr_p2.data,alpha=alpha)\n",
    "    arr_p12 = pi1*arr_p1 + pi2*arr_p2\n",
    "    H_12 = H_alpha_sparse_csr(arr_p12.data,alpha=alpha)\n",
    "\n",
    "    D = H_12 - pi1*H_1 - pi2*H_2\n",
    "\n",
    "    if normalized == False:\n",
    "        norm = 1.0\n",
    "    else:\n",
    "        norm = D_alpha_max(H_1,H_2,pi1,pi2,alpha=alpha)\n",
    "    D /= norm\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_f(path_to_counts):\n",
    "    \"\"\"\n",
    "    Reads a counts file, returns types and frequencies\n",
    "    \"\"\"\n",
    "    w_c_dict = get_dict_words_counts(path_to_counts)\n",
    "    if len(w_c_dict)==0:\n",
    "        return [],[]\n",
    "    w, c = np.array(list(w_c_dict.items())).T\n",
    "    c = c.astype(int)\n",
    "    L = np.sum(c)\n",
    "    if not isinstance(w,(list,np.ndarray)):\n",
    "        w=[w]\n",
    "        c=[c]\n",
    "    return w, c/L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the bookshelves data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "BS_df = pd.read_pickle(\"../data/bookshelves_clean.p\")\n",
    "\n",
    "# drop books we don't have\n",
    "books_we_have = map(lambda x: os.path.isfile(\"../../gutenberg/data/counts/%s_counts.txt\"%x), BS_df.index)\n",
    "BS_df = BS_df.iloc[list(books_we_have)]\n",
    "\n",
    "# limit to medium-large BS\n",
    "large_bookshelves = BS_df.loc[:, (BS_df.sum(axis=0)>50) & (BS_df.sum(axis=0)<150)  ].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Africa_(Bookshelf)\n",
      "Animals-Wild_(Bookshelf)-Birds\n",
      "Animals-Wild_(Bookshelf)-Mammals\n",
      "Art_(Bookshelf)\n",
      "Astounding_Stories_(Bookshelf)\n",
      "Australia_(Bookshelf)\n",
      "Biographies_(Bookshelf)\n",
      "Blackwood's_Edinburgh_Magazine_(Bookshelf)\n",
      "Canada_(Bookshelf)\n",
      "Chambers's_Edinburgh_Journal_(Bookshelf)\n",
      "Children's_History_(Bookshelf)\n",
      "Children's_Instructional_Books_(Bookshelf)\n",
      "Christmas_(Bookshelf)\n",
      "Classical_Antiquity_(Bookshelf)\n",
      "Contemporary_Reviews_(Bookshelf)\n",
      "Cookery_(Bookshelf)\n",
      "Crafts_(Bookshelf)\n",
      "Detective_Fiction_(Bookshelf)\n",
      "Fantasy_(Bookshelf)\n",
      "Germany_(Bookshelf)\n",
      "Harper's_Young_People_(Bookshelf)\n",
      "Horticulture_(Bookshelf)\n",
      "Judaism_(Bookshelf)\n",
      "Latter_Day_Saints_(Bookshelf)\n",
      "Masterpieces_in_Colour_(Bookshelf)\n",
      "Music_(Bookshelf)\n",
      "Native_America_(Bookshelf)\n",
      "Opera_(Bookshelf)\n",
      "Philosophy_(Bookshelf)\n",
      "Pirates,_Buccaneers,_Corsairs,_etc._(Bookshelf)\n",
      "Poetry_(Bookshelf)\n",
      "School_Stories_(Bookshelf)\n",
      "Scientific_American_(Bookshelf)\n",
      "Short_Stories_(Bookshelf)_Authors_F-J\n",
      "Slavery_(Bookshelf)\n",
      "The_American_Missionary_(Bookshelf)\n",
      "The_Atlantic_Monthly_(Bookshelf)\n",
      "Travel_(Bookshelf)\n",
      "United_States_(Bookshelf)\n",
      "Western_(Bookshelf)\n",
      "World_War_II_(Bookshelf)\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(large_bookshelves):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_projection(bookshelves):\n",
    "    books = np.concatenate([ BS_df[bs].dropna().index for bs in bookshelves])\n",
    "    labels = np.concatenate([ [bs]*len(BS_df[bs].dropna()) for bs in bookshelves])\n",
    "    paths = [\"../../gutenberg/data/counts/%s_counts.txt\"%b for b in books]\n",
    "    M = get_word2doc(paths)\n",
    "    u = umap.UMAP(n_neighbors=25, metric=D_alpha_sparse_csr, metric_kwds={\"alpha\":1}).fit_transform(M)\n",
    "    return u, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookshelves = [\"Judaism_(Bookshelf)\", \"Germany_(Bookshelf)\", \"Philosophy_(Bookshelf)\"]\n",
    "u, labels = compute_projection(bookshelves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1 ,figsize=(5, 4))\n",
    "for lab, sdf  in pd.DataFrame(u).groupby(labels):\n",
    "    ax.scatter(sdf[0], sdf[1], s=15, label=lab.replace(\"_(Bookshelf)\", \"\"))\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"UMAP0\")\n",
    "ax.set_ylabel(\"UMAP1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
