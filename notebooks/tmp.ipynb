{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "import umap\n",
    "import numba\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "sys.path.append(\"../../gutenberg/src\")\n",
    "from metaquery import meta_query\n",
    "from data_io import get_dict_words_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using UMAP with a precomputed distance matrix\n",
    "This is the simple approach but computing and storing a $50k \\times 50k$ matrix is no joke.\n",
    "\n",
    "### Using UMAP with a custom distance function\n",
    "Here we let UMAP handle the distances efficiently. Instead we need to pass:\n",
    "+ a **sparse** samples-features matrix\n",
    "+ a numba-jitted custom distance function\n",
    "\n",
    "Our features are word types, so the only way to hold the samples-features matrix is with sparse matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2doc(paths):\n",
    "    \"\"\"\n",
    "    Create a words-to-documents sparse matrix.\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    paths : list[str]\n",
    "        List of paths to count files\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    MM : scipy.sparse.csr_matrix\n",
    "        A csr sparse matrix with documents in rows and words in columns. \n",
    "        Entries are (relative) frequencies\n",
    "        \n",
    "    \"\"\"\n",
    "    !rm /tmp/PGtmp.txt\n",
    "    for path in paths:\n",
    "        ! cat {path} >> /tmp/PGtmp.txt\n",
    "\n",
    "    with open(\"/tmp/PGtmp.txt\",'r') as f:\n",
    "        x=f.readlines()\n",
    "    W = list(set([xx.split(\"\\t\")[0] for xx in x]))\n",
    "\n",
    "    M = dok_matrix((len(paths),len(W)))\n",
    "    # initialize map from words to integers\n",
    "    w_idx = {v:k for k,v in dict(enumerate(W)).items()}\n",
    "\n",
    "    for d, path in enumerate(paths):\n",
    "        w, f = get_w_f(path)\n",
    "\n",
    "        # add words to sparse matrix\n",
    "        for word,freq in zip(w, f):\n",
    "            M[d,w_idx[word]] = freq\n",
    "\n",
    "    return M.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def D_alpha_max(H1,H2,pi1,pi2,alpha=1.0):\n",
    "    '''The maximum Jensen-Shanon-divergence of two probability distributions p1 and p2\n",
    "    with entropies H1 and H2 and weights pi1 and pi2.\n",
    "    The maximum jsd is obtained by assuming the support of both distributions is disjunct.\n",
    "    For more information see:\n",
    "    Gerlach, Font-Clos, Altmann, Phys. Rev. X 6 (2016) 021009\n",
    "    https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.021009\n",
    "\n",
    "    INPUT:\n",
    "    - H1, float, alpha-entropy of p1\n",
    "    - H2, float, alpha-entropy of p2\n",
    "    - pi1, float, weight given to p1\n",
    "    - pi2, float, weight given to p2\n",
    "    optional\n",
    "    - alpha, float (default:1.0)\n",
    "\n",
    "    OUTPUT:\n",
    "    - D_max, float - maximum JSD\n",
    "    '''\n",
    "    D_max = 0.0\n",
    "    if alpha == 1.0:\n",
    "        D_max = -pi1*np.log(pi1)-pi2*np.log(pi2)\n",
    "    else:\n",
    "        D_max = (pi1**alpha-pi1)*H1+\\\n",
    "                (pi2**alpha-pi2)*H2+\\\n",
    "                1.0/(1.0-alpha)*(pi1**alpha+pi2**alpha-1)\n",
    "    return D_max\n",
    "\n",
    "@numba.njit()\n",
    "def H_alpha_sparse_csr(arr_p,alpha=1.0):\n",
    "    '''Calculate generalized entropy of order-alpha\n",
    "        H_{\\alpha}(\\vec{p}) = \\frac{1}{1-\\alpha}( \\sum_i p_i^{\\alpha} - 1  )\n",
    "        https://en.wikipedia.org/wiki/Tsallis_entropy\n",
    "\n",
    "        INPUT:\n",
    "        - arr_p, array (normalized probability distribution)\n",
    "        optional\n",
    "        - alpha, float (default:1.0 == Boltzmann-Shannon-Gibbs entropy)\n",
    "        OUT:\n",
    "        - H_alpha, float\n",
    "\n",
    "        Note that we have to consider special cases \n",
    "        - alpha=0; if p_i=0: p_i^0=0 and p_i>0: p_i^0=1  \n",
    "        - alpha=1; H --> -\\sum_i p_i log(p_i) and 0*log(0) = 0\n",
    "    '''\n",
    "    ## consider only entries with p>0\n",
    "    arr_p_pos = arr_p\n",
    "    H = 0.0\n",
    "    if alpha == 0.0:\n",
    "        H = len(arr_p_pos) - 1\n",
    "    elif alpha == 1.0:\n",
    "        H = -np.sum(arr_p_pos*np.log(arr_p_pos))\n",
    "    else:\n",
    "        H = 1.0/(1.0-alpha)*(np.sum(arr_p_pos**alpha) - 1.0)\n",
    "    return H\n",
    "\n",
    "def D_alpha_sparse_csr(arr_p1,arr_p2,alpha=2.0,pi1=0.5,normalized=True):\n",
    "    '''\n",
    "    Generalized Jensen-Shannon divergence proposed in:\n",
    "        Gerlach, Font-Clos, Altmann, Phys. Rev. X 6 (2016) 021009\n",
    "        https://journals.aps.org/prx/abstract/10.1103/PhysRevX.6.021009\n",
    "\n",
    "    quantifying the divergence between probability distributions p1 and p2\n",
    "\n",
    "    INPUT:\n",
    "    - p1, array  \n",
    "    - p2, array\n",
    "    optional\n",
    "    - alpha, float (default:1)\n",
    "    - pi1, float (default:0.5); weight given to p1 (pi2 = 1-pi1)\n",
    "    - normalized (default:False); if True return normalized version of alpha-JSD\n",
    "\n",
    "    OUTPUT:\n",
    "    - JSD, float\n",
    "\n",
    "    Note that p1 and p2 have to be defined over the SAME support, i.e.\n",
    "    for any index i the probabilities p1[i] and p2[i] refer to the same symbol.\n",
    "    If, for example a symbol j only appears in p1, then p1[j]>0 and p2[j]=0 (and vice versa)\n",
    "    '''\n",
    "\n",
    "    pi2 = 1.0-pi1\n",
    "\n",
    "    H_1 = H_alpha_sparse_csr(arr_p1.data,alpha=alpha)\n",
    "    H_2 = H_alpha_sparse_csr(arr_p2.data,alpha=alpha)\n",
    "    arr_p12 = pi1*arr_p1 + pi2*arr_p2\n",
    "    H_12 = H_alpha_sparse_csr(arr_p12.data,alpha=alpha)\n",
    "\n",
    "    D = H_12 - pi1*H_1 - pi2*H_2\n",
    "\n",
    "    if normalized == False:\n",
    "        norm = 1.0\n",
    "    else:\n",
    "        norm = D_alpha_max(H_1,H_2,pi1,pi2,alpha=alpha)\n",
    "    D /= norm\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_f(path_to_counts):\n",
    "    \"\"\"\n",
    "    Reads a counts file, returns types and frequencies\n",
    "    \"\"\"\n",
    "    w_c_dict = get_dict_words_counts(path_to_counts)\n",
    "    if len(w_c_dict)==0:\n",
    "        return [],[]\n",
    "    w, c = np.array(list(w_c_dict.items())).T\n",
    "    c = c.astype(int)\n",
    "    L = np.sum(c)\n",
    "    if not isinstance(w,(list,np.ndarray)):\n",
    "        w=[w]\n",
    "        c=[c]\n",
    "    return w, c/L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the bookshelves data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "BS_df = pd.read_pickle(\"../data/bookshelves.p\")\n",
    "\n",
    "# drop books we don't have\n",
    "books_we_have = map(lambda x: os.path.isfile(\"../../gutenberg/data/counts/%s_counts.txt\"%x), BS_df.index)\n",
    "BS_df = BS_df.iloc[list(books_we_have)]\n",
    "\n",
    "# limit to medium-large BS\n",
    "large_bookshelves = BS_df.loc[:, (BS_df.sum(axis=0)>50) & (BS_df.sum(axis=0)<100)  ].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    # choose 3 BS\n",
    "    bookshelves = np.random.choice(large_bookshelves, 8, replace=False)\n",
    "    bookshelves_colors_dict = {v:k for k,v in enumerate(bookshelves)}\n",
    "\n",
    "    books = np.concatenate([ BS_df[bs].dropna().index for bs in bookshelves])\n",
    "    colors = np.concatenate([ [bookshelves_colors_dict[bs]]*len(BS_df[bs].dropna()) for bs in bookshelves])\n",
    "    paths = [\"../../gutenberg/data/counts/%s_counts.txt\"%b for b in books]\n",
    "\n",
    "    M = get_word2doc(paths)\n",
    "\n",
    "    u1 = umap.UMAP(n_neighbors=25, metric=D_alpha_sparse_csr, metric_kwds={\"alpha\":1}).fit_transform(M)\n",
    "    u2 = umap.UMAP(n_neighbors=25, metric=D_alpha_sparse_csr, metric_kwds={\"alpha\":2}).fit_transform(M)\n",
    "\n",
    "    import matplotlib.cm as cm\n",
    "\n",
    "    fig, (ax1, ax2, bax) = plt.subplots(1,3,figsize=(18,6), gridspec_kw = {'width_ratios':[20, 20, 1]})\n",
    "\n",
    "    for ax, u, title in zip([ax1, ax2],[u1, u2], [\"alpha = 1\", \"alpha = 2\"]):\n",
    "        sc=ax.scatter(*u.T, s=15, c=colors, cmap=plt.cm.get_cmap(\"tab10\"), vmin=-0.5, vmax=9.5)\n",
    "        #ax.set_xlim(-10, 10)\n",
    "        ax.set_title(title, fontsize=16)\n",
    "\n",
    "    cbar = fig.colorbar(sc, cax=bax)\n",
    "    cbar.set_ticks(range(len(set(colors))))\n",
    "    cbar.set_ticklabels(bookshelves)\n",
    "    path = \"../figures/UMAP_alpha_1_2_\" + \"_\".join([x.split(\"_\")[0].replace(\"'\", \"\") for x in bookshelves]) + \".pdf\"\n",
    "    fig.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
